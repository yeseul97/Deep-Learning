{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multi-variable Linear Regression : 다항 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 3), (5, 1))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.random.set_seed(5)\n",
    "\n",
    "# 학습 데이터 : X,Y\n",
    "x_data = [[73.,80.,75.],\n",
    "          [93.,88.,93.],\n",
    "          [89.,91.,90.],\n",
    "          [96.,98.,100.],\n",
    "          [73.,66.,70.]]\n",
    "\n",
    "y_data = [[152.],\n",
    "          [185.],\n",
    "          [180.],\n",
    "          [196.],\n",
    "          [142.]]\n",
    "\n",
    "x_train = np.array(x_data,dtype=np.float32)  \n",
    "y_train = np.array(y_data,dtype=np.float32)\n",
    "x_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'weight:0' shape=(3, 1) dtype=float32, numpy=\n",
       "array([[-0.18030666],\n",
       "       [-0.95028627],\n",
       "       [-0.03964049]], dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 변수 초기화 : weight, bias\n",
    "# (m,n) * (n,l)  = (m,l)  , 행렬의 내적 곱셈 공식\n",
    "# (5,3) * (3,1)  = (5,1)\n",
    "\n",
    "W = tf.Variable(tf.random.normal([3,1]), name ='weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name = 'bias')\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypothesis 예측 함수 : H(X1,X2,X3) = W1*X1 + W2*X2 + W3*X3 + b\n",
    "def hypothesis(X):\n",
    "    return   tf.matmul(X,W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비용 함수 : (Hx - y)^2 의 평균\n",
    "# tf.square() : 제곱\n",
    "# tf.reduce_mean() : 합의 평균\n",
    "def cost_func():\n",
    "    cost = tf.reduce_mean(tf.square(hypothesis(x_train) - y_train))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사 하강법\n",
    "# learning_rate(학습율) 을 0.01로 설정하여 optimizer 객체를 생성\n",
    "# optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "# optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.01)\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Start Learning!!\n",
      "0000 cost: [ 72495.52 ] W: [[-0.17030665]\n",
      " [-0.9402863 ]\n",
      " [-0.02964049]] b: [0.23652864]\n",
      "0100 cost: [ 4484.4697 ] W: [[ 0.6139679 ]\n",
      " [-0.15593217]\n",
      " [ 0.754649  ]] b: [1.0209028]\n",
      "0200 cost: [ 39.03132 ] W: [[0.8497376 ]\n",
      " [0.08022431]\n",
      " [0.99049103]] b: [1.2571563]\n",
      "0300 cost: [ 1.8185713 ] W: [[0.8725854 ]\n",
      " [0.10369529]\n",
      " [1.0134547 ]] b: [1.2807844]\n",
      "0400 cost: [ 1.7926762 ] W: [[0.872841  ]\n",
      " [0.10471869]\n",
      " [1.0138539 ]] b: [1.2820021]\n",
      "0500 cost: [ 1.7869017 ] W: [[0.8724857 ]\n",
      " [0.10525568]\n",
      " [1.0136645 ]] b: [1.2827668]\n",
      "0600 cost: [ 1.780385 ] W: [[0.87208366]\n",
      " [0.10586476]\n",
      " [1.0134504 ]] b: [1.2836363]\n",
      "0700 cost: [ 1.7731373 ] W: [[0.87163574]\n",
      " [0.1065436 ]\n",
      " [1.0132116 ]] b: [1.2846082]\n",
      "0800 cost: [ 1.7651775 ] W: [[0.8711428 ]\n",
      " [0.10729134]\n",
      " [1.0129483 ]] b: [1.2856818]\n",
      "0900 cost: [ 1.7565353 ] W: [[0.87060463]\n",
      " [0.10810767]\n",
      " [1.0126605 ]] b: [1.2868583]\n",
      "1000 cost: [ 1.7471693 ] W: [[0.8700216 ]\n",
      " [0.10899291]\n",
      " [1.0123482 ]] b: [1.2881386]\n",
      "1100 cost: [ 1.7370999 ] W: [[0.86939305]\n",
      " [0.10994754]\n",
      " [1.012011  ]] b: [1.2895249]\n",
      "1200 cost: [ 1.726341 ] W: [[0.86871856]\n",
      " [0.11097253]\n",
      " [1.0116487 ]] b: [1.2910199]\n",
      "1300 cost: [ 1.7148823 ] W: [[0.8679975 ]\n",
      " [0.11206895]\n",
      " [1.0112597 ]] b: [1.2926265]\n",
      "1400 cost: [ 1.7027172 ] W: [[0.86722934]\n",
      " [0.11323832]\n",
      " [1.0108442 ]] b: [1.2943485]\n",
      "1500 cost: [ 1.6898162 ] W: [[0.866413  ]\n",
      " [0.11448204]\n",
      " [1.0104021 ]] b: [1.2961898]\n",
      "1600 cost: [ 1.6762359 ] W: [[0.8655475]\n",
      " [0.1158017]\n",
      " [1.009932 ]] b: [1.2981548]\n",
      "1700 cost: [ 1.6619005 ] W: [[0.86463165]\n",
      " [0.1171991 ]\n",
      " [1.0094329 ]] b: [1.3002483]\n",
      "1800 cost: [ 1.6468424 ] W: [[0.86366475]\n",
      " [0.11867622]\n",
      " [1.0089048 ]] b: [1.3024753]\n",
      "1900 cost: [ 1.6310679 ] W: [[0.86264545]\n",
      " [0.12023506]\n",
      " [1.0083456 ]] b: [1.3048418]\n",
      "2000 cost: [ 1.6145271 ] W: [[0.86157256]\n",
      " [0.12187756]\n",
      " [1.0077552 ]] b: [1.3073537]\n",
      "2100 cost: [ 1.5972543 ] W: [[0.86044496]\n",
      " [0.12360606]\n",
      " [1.0071325 ]] b: [1.310017]\n",
      "2200 cost: [ 1.5792168 ] W: [[0.8592615 ]\n",
      " [0.12542273]\n",
      " [1.0064763 ]] b: [1.3128389]\n",
      "2300 cost: [ 1.5604575 ] W: [[0.8580207 ]\n",
      " [0.12732972]\n",
      " [1.0057851 ]] b: [1.3158269]\n",
      "2400 cost: [ 1.5409226 ] W: [[0.85672176]\n",
      " [0.12932943]\n",
      " [1.005058  ]] b: [1.3189881]\n",
      "2500 cost: [ 1.520629 ] W: [[0.85536313]\n",
      " [0.13142417]\n",
      " [1.0042937 ]] b: [1.3223314]\n",
      "2600 cost: [ 1.4995819 ] W: [[0.8539439 ]\n",
      " [0.13361628]\n",
      " [1.003491  ]] b: [1.3258654]\n",
      "2700 cost: [ 1.4778014 ] W: [[0.85246265]\n",
      " [0.13590808]\n",
      " [1.0026494 ]] b: [1.3295994]\n",
      "2800 cost: [ 1.4552519 ] W: [[0.8509185 ]\n",
      " [0.13830191]\n",
      " [1.0017664 ]] b: [1.3335426]\n",
      "2900 cost: [ 1.4319923 ] W: [[0.8493104 ]\n",
      " [0.14080004]\n",
      " [1.0008415 ]] b: [1.337706]\n",
      "3000 cost: [ 1.4080118 ] W: [[0.8476374 ]\n",
      " [0.14340442]\n",
      " [0.99987257]] b: [1.3420995]\n",
      "3100 cost: [ 1.3833171 ] W: [[0.8458986 ]\n",
      " [0.14611748]\n",
      " [0.9988589 ]] b: [1.3467361]\n",
      "3200 cost: [ 1.3579305 ] W: [[0.84409356]\n",
      " [0.14894104]\n",
      " [0.9977988 ]] b: [1.3516272]\n",
      "3300 cost: [ 1.3318672 ] W: [[0.8422216 ]\n",
      " [0.15187693]\n",
      " [0.9966906 ]] b: [1.3567854]\n",
      "3400 cost: [ 1.3051581 ] W: [[0.8402824 ]\n",
      " [0.15492707]\n",
      " [0.995533  ]] b: [1.3622245]\n",
      "3500 cost: [ 1.2778144 ] W: [[0.8382758 ]\n",
      " [0.15809296]\n",
      " [0.9943242 ]] b: [1.367959]\n",
      "3600 cost: [ 1.2499115 ] W: [[0.83620167]\n",
      " [0.16137592]\n",
      " [0.99306303]] b: [1.374003]\n",
      "3700 cost: [ 1.2214508 ] W: [[0.8340602]\n",
      " [0.164777 ]\n",
      " [0.991748 ]] b: [1.3803726]\n",
      "3800 cost: [ 1.1924527 ] W: [[0.83185214]\n",
      " [0.16829714]\n",
      " [0.9903773 ]] b: [1.3870845]\n",
      "3900 cost: [ 1.1630199 ] W: [[0.8295782 ]\n",
      " [0.17193674]\n",
      " [0.9889496 ]] b: [1.3941557]\n",
      "4000 cost: [ 1.133139 ] W: [[0.82723945]\n",
      " [0.17569605]\n",
      " [0.9874634 ]] b: [1.4016045]\n",
      "4100 cost: [ 1.1029172 ] W: [[0.8248373 ]\n",
      " [0.17957495]\n",
      " [0.9859173 ]] b: [1.4094498]\n",
      "4200 cost: [ 1.072374 ] W: [[0.8223736 ]\n",
      " [0.18357278]\n",
      " [0.9843098 ]] b: [1.4177122]\n",
      "4300 cost: [ 1.0415707 ] W: [[0.8198507 ]\n",
      " [0.18768853]\n",
      " [0.9826393 ]] b: [1.426412]\n",
      "4400 cost: [ 1.0105848 ] W: [[0.81727105]\n",
      " [0.19192071]\n",
      " [0.9809047 ]] b: [1.435571]\n",
      "4500 cost: [ 0.97949696 ] W: [[0.8146378 ]\n",
      " [0.19626723]\n",
      " [0.9791045 ]] b: [1.4452116]\n",
      "4600 cost: [ 0.94836605 ] W: [[0.81195456]\n",
      " [0.20072547]\n",
      " [0.9772375 ]] b: [1.4553584]\n",
      "4700 cost: [ 0.9172562 ] W: [[0.8092256]\n",
      " [0.2052924]\n",
      " [0.9753026]] b: [1.4660355]\n",
      "4800 cost: [ 0.8862562 ] W: [[0.8064553 ]\n",
      " [0.20996396]\n",
      " [0.97329843]] b: [1.4772686]\n",
      "4900 cost: [ 0.85545003 ] W: [[0.80364925]\n",
      " [0.214736  ]\n",
      " [0.9712242 ]] b: [1.4890838]\n",
      "5000 cost: [ 0.8249054 ] W: [[0.8008126 ]\n",
      " [0.21960317]\n",
      " [0.9690788 ]] b: [1.5015098]\n",
      "5100 cost: [ 0.79471046 ] W: [[0.79795206]\n",
      " [0.22455971]\n",
      " [0.96686155]] b: [1.5145746]\n",
      "5200 cost: [ 0.7649435 ] W: [[0.79507494]\n",
      " [0.22959898]\n",
      " [0.9645713 ]] b: [1.5283078]\n",
      "5300 cost: [ 0.73568785 ] W: [[0.7921892 ]\n",
      " [0.23471346]\n",
      " [0.9622074 ]] b: [1.542739]\n",
      "5400 cost: [ 0.70699435 ] W: [[0.7893028 ]\n",
      " [0.23989508]\n",
      " [0.95976937]] b: [1.5578997]\n",
      "5500 cost: [ 0.67898214 ] W: [[0.786424  ]\n",
      " [0.24513553]\n",
      " [0.9572569 ]] b: [1.5738223]\n",
      "5600 cost: [ 0.6516709 ] W: [[0.78356296]\n",
      " [0.25042507]\n",
      " [0.9546696 ]] b: [1.5905393]\n",
      "5700 cost: [ 0.6251567 ] W: [[0.7807289 ]\n",
      " [0.25575364]\n",
      " [0.9520071 ]] b: [1.6080831]\n",
      "5800 cost: [ 0.5994874 ] W: [[0.77793276]\n",
      " [0.2611102 ]\n",
      " [0.94926995]] b: [1.6264883]\n",
      "5900 cost: [ 0.57471454 ] W: [[0.7751856 ]\n",
      " [0.26648337]\n",
      " [0.9464575 ]] b: [1.645788]\n",
      "6000 cost: [ 0.5508672 ] W: [[0.77249897]\n",
      " [0.2718607 ]\n",
      " [0.94357055]] b: [1.666017]\n",
      "6100 cost: [ 0.52799714 ] W: [[0.7698844 ]\n",
      " [0.2772301 ]\n",
      " [0.94060886]] b: [1.687209]\n",
      "6200 cost: [ 0.5061166 ] W: [[0.76735437]\n",
      " [0.28257856]\n",
      " [0.9375731 ]] b: [1.7093982]\n",
      "6300 cost: [ 0.48523998 ] W: [[0.76492107]\n",
      " [0.28789258]\n",
      " [0.9344636 ]] b: [1.7326185]\n",
      "6400 cost: [ 0.46538228 ] W: [[0.7625969 ]\n",
      " [0.29315904]\n",
      " [0.93128127]] b: [1.7569026]\n",
      "6500 cost: [ 0.44651723 ] W: [[0.7603942]\n",
      " [0.2983643]\n",
      " [0.9280264]] b: [1.7822821]\n",
      "6600 cost: [ 0.42864066 ] W: [[0.75832546]\n",
      " [0.30349517]\n",
      " [0.92469996]] b: [1.8087882]\n",
      "6700 cost: [ 0.41173983 ] W: [[0.7564021 ]\n",
      " [0.30853832]\n",
      " [0.9213029 ]] b: [1.8364505]\n",
      "6800 cost: [ 0.39574704 ] W: [[0.75463605]\n",
      " [0.31348118]\n",
      " [0.91783595]] b: [1.8652945]\n",
      "6900 cost: [ 0.38066036 ] W: [[0.753038  ]\n",
      " [0.31831154]\n",
      " [0.9143001 ]] b: [1.8953496]\n",
      "7000 cost: [ 0.36640996 ] W: [[0.7516179 ]\n",
      " [0.32301787]\n",
      " [0.9106965 ]] b: [1.9266357]\n",
      "7100 cost: [ 0.35295027 ] W: [[0.7503854 ]\n",
      " [0.3275899 ]\n",
      " [0.90702575]] b: [1.9591726]\n",
      "7200 cost: [ 0.34020546 ] W: [[0.74934834]\n",
      " [0.33201808]\n",
      " [0.9032893 ]] b: [1.9929783]\n",
      "7300 cost: [ 0.3281333 ] W: [[0.7485137 ]\n",
      " [0.3362941 ]\n",
      " [0.89948833]] b: [2.0280635]\n",
      "7400 cost: [ 0.3166648 ] W: [[0.74788713]\n",
      " [0.34041137]\n",
      " [0.8956236 ]] b: [2.0644376]\n",
      "7500 cost: [ 0.3057421 ] W: [[0.7474729 ]\n",
      " [0.34436437]\n",
      " [0.8916965 ]] b: [2.102103]\n",
      "7600 cost: [ 0.2952928 ] W: [[0.7472732 ]\n",
      " [0.34814948]\n",
      " [0.887708  ]] b: [2.1410587]\n",
      "7700 cost: [ 0.28529948 ] W: [[0.74728906]\n",
      " [0.3517649 ]\n",
      " [0.8836596 ]] b: [2.1812973]\n",
      "7800 cost: [ 0.27565715 ] W: [[0.7475199 ]\n",
      " [0.35520983]\n",
      " [0.8795521 ]] b: [2.2228067]\n",
      "7900 cost: [ 0.26636675 ] W: [[0.747963 ]\n",
      " [0.3584857]\n",
      " [0.8753877]] b: [2.2655659]\n",
      "8000 cost: [ 0.2573676 ] W: [[0.7486139 ]\n",
      " [0.36159536]\n",
      " [0.8711678 ]] b: [2.3095496]\n",
      "8100 cost: [ 0.24863775 ] W: [[0.74946666]\n",
      " [0.36454386]\n",
      " [0.8668938 ]] b: [2.3547254]\n",
      "8200 cost: [ 0.24013594 ] W: [[0.75051385]\n",
      " [0.36733687]\n",
      " [0.86256784]] b: [2.4010491]\n",
      "8300 cost: [ 0.2318714 ] W: [[0.7517459 ]\n",
      " [0.3699816 ]\n",
      " [0.85819286]] b: [2.448475]\n",
      "8400 cost: [ 0.22381142 ] W: [[0.75315243]\n",
      " [0.37248656]\n",
      " [0.8537714 ]] b: [2.4969454]\n",
      "8500 cost: [ 0.21595547 ] W: [[0.75472164]\n",
      " [0.374861  ]\n",
      " [0.8493071 ]] b: [2.5463955]\n",
      "8600 cost: [ 0.20830786 ] W: [[0.7564404 ]\n",
      " [0.37711468]\n",
      " [0.8448037 ]] b: [2.5967522]\n",
      "8700 cost: [ 0.20087352 ] W: [[0.75829536]\n",
      " [0.37925786]\n",
      " [0.8402652 ]] b: [2.6479347]\n",
      "8800 cost: [ 0.19365798 ] W: [[0.7602717 ]\n",
      " [0.38130048]\n",
      " [0.8356975 ]] b: [2.6998537]\n",
      "8900 cost: [ 0.18666992 ] W: [[0.7623547 ]\n",
      " [0.38325307]\n",
      " [0.8311059 ]] b: [2.7524118]\n",
      "9000 cost: [ 0.17993662 ] W: [[0.7645293 ]\n",
      " [0.38512504]\n",
      " [0.82649755]] b: [2.8055027]\n",
      "9100 cost: [ 0.17345294 ] W: [[0.76678026]\n",
      " [0.386925  ]\n",
      " [0.8218793 ]] b: [2.859013]\n",
      "9200 cost: [ 0.16724154 ] W: [[0.76909244]\n",
      " [0.3886619 ]\n",
      " [0.8172601 ]] b: [2.9128203]\n",
      "9300 cost: [ 0.16132697 ] W: [[0.77145123]\n",
      " [0.3903423 ]\n",
      " [0.8126484 ]] b: [2.9667995]\n",
      "9400 cost: [ 0.15571044 ] W: [[0.773842  ]\n",
      " [0.39197206]\n",
      " [0.8080548 ]] b: [3.0208173]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9500 cost: [ 0.15040575 ] W: [[0.7762509 ]\n",
      " [0.39355597]\n",
      " [0.80348957]] b: [3.0747292]\n",
      "9600 cost: [ 0.14542289 ] W: [[0.7786646 ]\n",
      " [0.39509755]\n",
      " [0.79896426]] b: [3.1283932]\n",
      "9700 cost: [ 0.14076935 ] W: [[0.78107023]\n",
      " [0.39659917]\n",
      " [0.794491  ]] b: [3.1816597]\n",
      "9800 cost: [ 0.13645592 ] W: [[0.7834555 ]\n",
      " [0.39806178]\n",
      " [0.7900828 ]] b: [3.2343774]\n",
      "9900 cost: [ 0.13246866 ] W: [[0.7858089 ]\n",
      " [0.39948562]\n",
      " [0.78575236]] b: [3.2863927]\n",
      "10000 cost: [ 0.12882271 ] W: [[0.78812015]\n",
      " [0.40086982]\n",
      " [0.7815125 ]] b: [3.3375518]\n",
      "***** Learning Finished\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작\n",
    "print('***** Start Learning!!')\n",
    "for step in range(10001):\n",
    "    # cost를 minimize한다\n",
    "    optimizer.minimize(cost_func,var_list=[W,b])\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        print('%04d'%step,'cost: [',cost_func().numpy(),']',\n",
    "             'W:',W.numpy(),'b:',b.numpy())\n",
    "\n",
    "print('***** Learning Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight : [[0.78812015]\n",
      " [0.40086982]\n",
      " [0.7815125 ]]\n",
      "Bias: [3.3375518]\n"
     ]
    }
   ],
   "source": [
    "# 회귀 계수 : weight과 bias출력\n",
    "print('Weight :',W.numpy())\n",
    "print('Bias:',b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Predict\n",
      "[[151.55334]\n",
      " [184.58994]\n",
      " [180.29553]\n",
      " [196.43358]\n",
      " [142.03362]]\n"
     ]
    }
   ],
   "source": [
    "# 예측\n",
    "print('***** Predict')\n",
    "x_data = [[73.,80.,75.],\n",
    "          [93.,88.,93.],\n",
    "          [89.,91.,90.],\n",
    "          [96.,98.,100.],\n",
    "          [73.,66.,70.]]\n",
    "x_test = np.array(x_data,dtype=np.float32)\n",
    "print(hypothesis(x_test).numpy())\n",
    "# [[151.55334]\n",
    "#  [184.58994]\n",
    "#  [180.29553]\n",
    "#  [196.43358]\n",
    "#  [142.03362]]\n",
    "\n",
    "# y_data = [[152.],\n",
    "#           [185.],\n",
    "#           [180.],\n",
    "#           [196.],\n",
    "#           [142.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
