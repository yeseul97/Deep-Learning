{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax_multi_classification\n",
    "# multi-nomial Classification (다중 분류) : Y값의 범주가 3개 이상인 분류\n",
    "# 활성화 함수(Activation function) 으로 softmax()가 사용 된다\n",
    "# 회귀 모델(일차함수)로 가림막을 3가지 침(분류갯수=3이면)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.random.set_seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data set :\n",
    "# x_data :  [N,4]  --> [8,4]\n",
    "x_data = [[1,2,1,1],\n",
    "          [2,1,3,2],\n",
    "          [3,1,3,4],\n",
    "          [4,1,5,5],\n",
    "          [1,7,5,5],\n",
    "          [1,2,5,6],\n",
    "          [1,6,6,6],\n",
    "          [1,7,7,7]]\n",
    "\n",
    "# y_data : [N,3] --> [8,3]\n",
    "y_data = [[0,0,1],  # [2]\n",
    "          [0,0,1],  # [2]\n",
    "          [0,0,1],  # [2]\n",
    "          [0,1,0],  # [1]\n",
    "          [0,1,0],  # [1]\n",
    "          [0,1,0],  # [1]\n",
    "          [1,0,0],  # [0]\n",
    "          [1,0,0]]  # [0]\n",
    "# one hot 인코딩\n",
    "\n",
    "x_train = np.array(x_data,dtype=np.float32)\n",
    "y_train = np.array(y_data,dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 초기화 : weight, bias\n",
    "# (8,3) *(4,3) = (8,3)\n",
    "nb_classes = 3  # 분류 갯수\n",
    "\n",
    "W = tf.Variable(tf.random.normal([4,nb_classes]), name ='weight')\n",
    "b = tf.Variable(tf.random.normal([nb_classes]), name = 'bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypothesis 예측 함수 : H(X) = softmax(W*X + b)\n",
    "def logits(X):\n",
    "    return tf.matmul(X,W) + b\n",
    "\n",
    "def hypothesis(X):\n",
    "    return   tf.nn.softmax(logits(X))\n",
    "# 위 함수 반환값을 아래 함수에서 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 비용 함수 구현 방법 1 : log함수를 사용하여 수식을 직접 표현\n",
    "# def cost_func():\n",
    "#     cost = tf.reduce_mean(-tf.reduce_sum(y_train*tf.math.log(hypothesis(x_train)),axis=1))\n",
    "    \n",
    "#     return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비용 함수 구현 방법 2 : tf.nn.softmax_cross_entropy_with_logits() 함수 사용\n",
    "def cost_func():\n",
    "    cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits(x_train),\n",
    "                                                     labels=y_train)\n",
    "    cost = tf.reduce_mean(cost_i)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사 하강법\n",
    "# learning_rate(학습율) 을 0.01로 설정하여 optimizer 객체를 생성\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Start Learning!!\n",
      "0000 cost: [ 5.9294786 ] W: [[-0.17030679 -0.94028634 -0.04964045]\n",
      " [-0.7325406   1.3331522  -0.628548  ]\n",
      " [ 0.86406636 -0.07899956  2.4388697 ]\n",
      " [ 0.77250797  1.2759615   0.9701489 ]] b: [0.2365285  0.8206551  0.73660946]\n",
      "1000 cost: [ 0.31467578 ] W: [[-2.2194247e+00  5.2480620e-01  1.1872505e+00]\n",
      " [ 1.2104916e-01  2.2530602e-03 -6.3700572e-02]\n",
      " [ 2.2870739e+00  1.1651016e+00  6.7435086e-01]\n",
      " [ 1.2760866e+00  1.7768894e+00  3.7673813e-01]] b: [-2.71877   -1.5067686  4.317265 ]\n",
      "2000 cost: [ 0.16304699 ] W: [[-3.7972443   1.3537493   2.267659  ]\n",
      " [ 0.0414911   0.02373775  0.06498498]\n",
      " [ 3.5885108   1.0490662  -0.50219786]\n",
      " [ 1.0556871   1.817029    0.5562779 ]] b: [-6.1154466 -1.3375344  7.16916  ]\n",
      "3000 cost: [ 0.08965951 ] W: [[-5.2343016   2.1662533   3.2086897 ]\n",
      " [-0.04391001  0.06965797  0.16019413]\n",
      " [ 4.8728337   0.69199216 -1.5755051 ]\n",
      " [ 0.73721164  2.0572202   0.7426419 ]] b: [-9.075828  -1.3604162  9.762421 ]\n",
      "4000 cost: [ 0.05118698 ] W: [[-6.569132    2.9417915   4.072178  ]\n",
      " [-0.12962751  0.1206399   0.24656829]\n",
      " [ 6.118547    0.27676854 -2.5946245 ]\n",
      " [ 0.3753315   2.3316689   0.953906  ]] b: [-11.740847   -1.3668194  12.113414 ]\n",
      "5000 cost: [ 0.029821288 ] W: [[-7.8441830e+00  3.6925721e+00  4.8953791e+00]\n",
      " [-2.1366470e-01  1.7167602e-01  3.2927620e-01]\n",
      " [ 7.3285356e+00 -1.4589632e-01 -3.5808389e+00]\n",
      " [-2.5742422e-03  2.6041079e+00  1.1803545e+00]] b: [-14.22404    -1.3462191  14.310619 ]\n",
      "***** Learning Finished\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작\n",
    "print('***** Start Learning!!')\n",
    "for step in range(5001):\n",
    "    # cost를 minimize한다\n",
    "    optimizer.minimize(cost_func,var_list=[W,b])\n",
    "    \n",
    "    if step % 1000 == 0:\n",
    "        print('%04d'%step,'cost: [',cost_func().numpy(),']',\n",
    "             'W:',W.numpy(),'b:',b.numpy())\n",
    "\n",
    "print('***** Learning Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight : [[-7.8441830e+00  3.6925721e+00  4.8953791e+00]\n",
      " [-2.1366470e-01  1.7167602e-01  3.2927620e-01]\n",
      " [ 7.3285356e+00 -1.4589632e-01 -3.5808389e+00]\n",
      " [-2.5742422e-03  2.6041079e+00  1.1803545e+00]]\n",
      "Bias: [-14.22404    -1.3462191  14.310619 ]\n"
     ]
    }
   ],
   "source": [
    "# 회귀 계수 : weight과 bias출력\n",
    "print('Weight :',W.numpy())\n",
    "print('Bias:',b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Predict\n",
      "[2 2 2 1 1 1 0 0]\n",
      "[[6.7201891e-15 4.4788094e-06 9.9999547e-01]\n",
      " [3.0885475e-11 6.2573748e-03 9.9374264e-01]\n",
      " [8.2859493e-18 3.1583805e-02 9.6841621e-01]\n",
      " [5.7242741e-16 9.7510344e-01 2.4896599e-02]\n",
      " [5.8690630e-02 9.3963599e-01 1.6734032e-03]\n",
      " [3.0668104e-02 9.6914285e-01 1.8900630e-04]\n",
      " [9.2271829e-01 7.7280879e-02 9.1235347e-07]\n",
      " [9.9905401e-01 9.4603549e-04 1.0147023e-10]]\n",
      "[2 2 2 1 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# 예측\n",
    "# tf.argmax() : 값이 가장 큰 요소의 인덱스 값을 반환\n",
    "def predict(X):\n",
    "    return tf.argmax(hypothesis(X),axis=1)\n",
    "\n",
    "print('***** Predict')\n",
    "\n",
    "# 학습데이터를 그대로 검증데이터로 사용한 경우\n",
    "x_test = np.array(x_data,dtype=np.float32)\n",
    "y_test = np.array(y_data,dtype=np.float32)\n",
    "\n",
    "preds = predict(x_test)\n",
    "print(preds.numpy())\n",
    "print(hypothesis(x_test).numpy())\n",
    "print(tf.argmax(y_test,1).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
